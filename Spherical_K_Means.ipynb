{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Spherical K-Means.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e987300ade6d4c4d9b6d4d09228a4d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2d0e50308adf42da8f5f65963a1af50a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3059921ee908434382c0b9748dc6a767",
              "IPY_MODEL_bcf7aa0066dc45f7b6341b18fed44a88"
            ]
          }
        },
        "2d0e50308adf42da8f5f65963a1af50a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3059921ee908434382c0b9748dc6a767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2af9251e1d3c4460b6ae2fda13ae4724",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed94f88f9ece40b795b9e7a33e8c4033"
          }
        },
        "bcf7aa0066dc45f7b6341b18fed44a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_61dade7ba51a49a9b1e88180ccdc120a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1/1 [00:01&lt;00:00,  1.00s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a65b35a1d95f4fadb81979a94bf32aed"
          }
        },
        "2af9251e1d3c4460b6ae2fda13ae4724": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed94f88f9ece40b795b9e7a33e8c4033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61dade7ba51a49a9b1e88180ccdc120a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a65b35a1d95f4fadb81979a94bf32aed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethical-stomach"
      },
      "source": [
        "# Spherical K-Means  \n",
        "\n",
        "Code taken from:  \n",
        "Dan Stowell and Mark Plumbley (2014), Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning  \n",
        "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4106198/ \n"
      ],
      "id": "ethical-stomach"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "visible-performer"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "# Online spherical Hartigan k-means\n",
        "# Dan Stowell, Jan 2014\n",
        "\n",
        "# This file in particular is published under the following open licence:\n",
        "#######################################################################################\n",
        "# Copyright (c) 2014, Dan Stowell and Queen Mary University of London\n",
        "# All rights reserved.\n",
        "#\n",
        "# Redistribution and use in source and binary forms, with or without modification, are permitted provided\n",
        "# that the following conditions are met:\n",
        "#\n",
        "# 1. Redistributions of source code must retain the above copyright notice, this list of conditions\n",
        "# and the following disclaimer.\n",
        "#\n",
        "# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions\n",
        "#    and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
        "#\n",
        "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED \n",
        "# WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n",
        "# PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR\n",
        "# ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\n",
        "# TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n",
        "# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n",
        "# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
        "# POSSIBILITY OF SUCH DAMAGE.\n",
        "#######################################################################################\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class OSKmeans:\n",
        "    \"\"\"\n",
        "    This class implements online Hartigan k-means, except:\n",
        "     - spherical k-means, i.e. the centroids are constrained to L2-norm of 1\n",
        "     - centroids are initialised randomly on the sphere\n",
        "     - a weight-offset is used to ensure that the random inits aren't simply blown away as soon as the first data-point comes in\n",
        "\n",
        "    Further reading:\n",
        "     - Appendix B of         http://cseweb.ucsd.edu/~bmcfee/papers/bmcfee_dissertation.pdf\n",
        "     - Coates et al 2012, Learning feature representations with k-means\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, k, d, weightoffset=2):\n",
        "        \"\"\"\n",
        "        k = num centroids\n",
        "        d = num dimensions\n",
        "        weightoffset: how \"reluctant\" the centroids are to move at first. set to 0 for traditional initialisation of centroids as random data.\n",
        "        \"\"\"\n",
        "        self.hitcounts =          [weightoffset               for _ in range(k)]     # shape (k)\n",
        "        self.centroids = np.array([spherical_random_sample(d) for _ in range(k)])    # shape (k, d)\n",
        "        self.k = k\n",
        "        self.d = d\n",
        "\n",
        "    def weightednearest(self, datum):\n",
        "        \"Find the index of the best-matching centroid, using the Hartigan count-weighting scheme in combination with cosine similarity\"\n",
        "        # find the best-matching centroid (note that it's not pure distance, but count-weighted)\n",
        "        bestindex = 0\n",
        "        bestval   = np.inf\n",
        "        bestcosinesim   = 0\n",
        "        datum = unit_normalise(datum)\n",
        "\n",
        "        try:\n",
        "            cosinesims = np.dot(self.centroids, datum.T)  # cosine similarity, shape (k)\n",
        "            cosinedists = 1. - cosinesims  # converted to a distance for the benefit of the count-weighting\n",
        "            cosinedists *= [self.hitcounts[which]/float(self.hitcounts[which]+1) for which in range(self.k)]  # Hartigan count-weighting\n",
        "        except:\n",
        "            print(\"Matrix shapes were: centroids %s, datum %s\" % (np.shape(self.centroids), np.shape(datum)))\n",
        "            raise\n",
        "\n",
        "        bestindex = np.argmin(cosinedists)\n",
        "        bestcosinesim = float(cosinesims[bestindex])\n",
        "        return (bestindex, bestcosinesim)  # NOTE: the dot product returned here is with the normalised input, i.e. with its unit vector.\n",
        "\n",
        "    def update(self, datum):\n",
        "        \"Feed individual data into this to perform learning\"\n",
        "        datum = np.array(datum)\n",
        "        bestindex, dotprod = self.weightednearest(datum)\n",
        "\n",
        "        # update the centroid, including the renormalisation for sphericalness\n",
        "        centroid = self.centroids[bestindex]\n",
        "        hitcount = self.hitcounts[bestindex]\n",
        "        newcentroid = unit_normalise(centroid * hitcount + datum * dotprod)\n",
        "        self.centroids[bestindex] = newcentroid\n",
        "\n",
        "        # update the hit count\n",
        "        self.hitcounts[bestindex] = hitcount + 1\n",
        "        # return the index, and the amount by which the centroid has changed (useful for monitoring)\n",
        "        return (bestindex, np.sqrt(((centroid-newcentroid)**2).sum()))\n",
        "\n",
        "    def train_batch(self, whitedata, niters=10, verbose=True):\n",
        "        \"If you have a batch of data, rather than streamed, this method is a convenience to train using 'niters' iterations of the shuffled data.\"\n",
        "        shuffle_indices = range(len(whitedata))\n",
        "        for whichiter in range(niters):\n",
        "            if verbose:\n",
        "                print (f\"Iteration {whichiter}\")\n",
        "            np.random.shuffle(shuffle_indices)\n",
        "            for upindex, index in enumerate(shuffle_indices):\n",
        "                self.update(whitedata[index])\n",
        "\n",
        "    def sort_centroids(self):\n",
        "        \"\"\"Not needed! Purely cosmetic, for comprehensibility of certain plots. Reorders the centroids by an arbitrary spectral-centroid-like measure.\n",
        "        Note that for 2D features such as chrmfull, the ordering may not have obvious sense since it operates on the vectorised data.\"\"\"\n",
        "        if False:\n",
        "            binlist = np.arange(len(self.centroids[0]))\n",
        "            sortifier = np.argsort([np.sum(centroid * binlist)/np.sum(centroid) for centroid in self.centroids])\n",
        "        else:\n",
        "            # new sorting method, using a simple approximation to TSP to organise the centroids so that close ones are close.\n",
        "            similarities = np.dot(self.centroids, self.centroids.T)\n",
        "            #print \"sort_centroids() -- similarities is of shape %s\" % str(similarities.shape)\n",
        "\n",
        "            # Now, starting arbitrarily from index 0, we \"grow each end\" iteratively\n",
        "            pathends = [[0], [0]]\n",
        "            worstarcpos = None\n",
        "            worstarcdot = 1\n",
        "            availablenodes = range(1, len(similarities))\n",
        "            whichend = 0\n",
        "            #print(\"---------------------------------------------------\")\n",
        "            while len(availablenodes) != 0:\n",
        "                #print(\"availablenodes (length %i): %s\" % (len(availablenodes), str(availablenodes)))\n",
        "                whichend = 1 - whichend  # alternate between one and zero\n",
        "                frm = pathends[whichend][-1]\n",
        "                # iterate over the nodes that are so-far unused, finding the best one to join on\n",
        "                bestpos = availablenodes[0]\n",
        "                bestdot = -1\n",
        "                for too in availablenodes:\n",
        "                    curdot = similarities[frm, too]\n",
        "                    if curdot > bestdot:\n",
        "                        bestpos = too\n",
        "                        bestdot = curdot\n",
        "                # check if this is the worst arc so far made\n",
        "                if bestdot < worstarcdot:\n",
        "                    worstarcdot = bestdot\n",
        "                    worstarcpos = (whichend, len(pathends[whichend]))\n",
        "                # append this arc\n",
        "                pathends[whichend].append(bestpos)\n",
        "                # remove the chosen one from availablenodes\n",
        "                #print(\" bestpos: %i, dot %g\" % (bestpos, bestdot))\n",
        "                availablenodes.remove(bestpos)\n",
        "\n",
        "            # finally, we need to check the join-the-two-ends arc to see if it's the worst\n",
        "            curdot = similarities[pathends[0][-1], pathends[1][-1]]\n",
        "            # we can choose the worst arc found as the place to split the circuit; and create the sortifier\n",
        "            if curdot < worstarcdot:\n",
        "                # we will snip the way the paths themselves snipped\n",
        "                sortifier = pathends[0][::-1] + pathends[1][1:]\n",
        "            else:\n",
        "                # we will snip at some location inside one of the lists, and rejoin\n",
        "                (snipwhich, snipwhere) = worstarcpos\n",
        "                sortifier = pathends[snipwhich][snipwhere::-1] + pathends[1-snipwhich][1:] + pathends[snipwhich][:snipwhere:-1]\n",
        "            if sorted(sortifier) != range(len(similarities)):\n",
        "                print(\"pathends: %s\" % str(pathends))\n",
        "                raise RuntimeError(\"sorted(sortifier) != range(len(similarities)): sorted(%s) != range(%s)\") % (sortifier, len(similarities))\n",
        "            #print(\"Simple TSP method: decided on the following sortifier: %s\" % str(sortifier))\n",
        "\n",
        "        self.centroids = np.array([self.centroids[index] for index in sortifier])\n",
        "        self.hitcounts =          [self.hitcounts[index] for index in sortifier]\n",
        "\n",
        "\n",
        "    def relative_entropy_hitcounts(self):\n",
        "        \"The entropy over the centroid hitcounts is a useful measure of how well they are used. Here we normalise it against the ideal uniform entropy\"\n",
        "        h = 0.\n",
        "        tot = float(np.sum(self.hitcounts))\n",
        "        for hitcount in self.hitcounts:\n",
        "            p = hitcount / tot\n",
        "            h -= p * np.log(p)\n",
        "        h_unif = np.log(len(self.hitcounts))\n",
        "        return h / h_unif\n",
        "\n",
        "    def reconstruct1(self, datum, whichcentroid):\n",
        "        \"Reconstruct an input datum using a single indexed centroid\"\n",
        "        return self.centroids[whichcentroid] * np.dot(self.centroids[whichcentroid], datum)\n",
        "\n",
        "    def dotproducts(self, data):\n",
        "        'Used by thresholded_dotproducts(); subclasses may overwrite'\n",
        "        return np.dot(data, self.centroids.T)\n",
        "\n",
        "    def thresholded_dotproducts(self, data, threshold=0.0):\n",
        "        \"One possible 'feature' set based on centroids is this, the thresholded dot products. Supply a matrix as one row per datum.\"\n",
        "        try:\n",
        "            return np.maximum(0, self.dotproducts(data) - threshold)\n",
        "        except:\n",
        "            print(\"Matrix shapes were: centroids %s, data %s\" % (np.shape(self.centroids), np.shape(data)))\n",
        "            raise\n",
        "\n",
        "###############################################\n",
        "def spherical_random_sample(d):\n",
        "    vec = np.random.normal(size=d)\n",
        "    return unit_normalise(vec)\n",
        "\n",
        "def unit_normalise(vec):\n",
        "    return vec / np.sqrt((vec ** 2).sum())\n",
        "\n",
        "###############################################\n",
        "# useful functions for whitening a dataset\n",
        "\n",
        "def normalise_and_whiten(data, retain=0.99, bias=1e-8, use_selfnormn=True, min_ndims=1):\n",
        "    \"Use this to prepare a training set before running through OSKMeans\"\n",
        "    mean = np.mean(data, 0)\n",
        "    normdata = data - mean\n",
        "\n",
        "    if use_selfnormn:\n",
        "        for i in range(normdata.shape[0]):\n",
        "            normdata[i] -= np.mean(normdata[i])\n",
        "\n",
        "    # this snippet is based on an example by Sander Dieleman\n",
        "    cov = np.dot(normdata.T, normdata) / normdata.shape[0]\n",
        "    eigs, eigv = np.linalg.eigh(cov) # docs say the eigenvalues are NOT NECESSARILY ORDERED, but this seems to be the case in practice...\n",
        "    print (f\"computing number of components to retain {retain:.2f} of the variance...\")\n",
        "    normed_eigs = eigs[::-1] / np.sum(eigs) # maximal value first\n",
        "    eigs_sum = np.cumsum(normed_eigs)\n",
        "    num_components = max(min_ndims, np.argmax(eigs_sum > retain)) # argmax selects the first index where eigs_sum > retain is true\n",
        "    print(f\"number of components to retain: {num_components} of {len(eigs)}\")\n",
        "    P = eigv.astype('float32') * np.sqrt(1.0/(eigs + bias)) # PCA whitening\n",
        "    P = P[:, -num_components:] # truncate transformation matrix\n",
        "\n",
        "    whitedata = np.dot(normdata, P)\n",
        "    invproj = np.linalg.pinv(P)\n",
        "    return ({'centre': mean, 'proj': P, 'ncomponents': num_components, 'invproj': invproj, 'use_selfnormn': use_selfnormn}, whitedata)\n",
        "\n",
        "def prepare_data(data, norminfo):\n",
        "    \"Typically used for new data; you use normalise_and_whiten() on your training data, then this method projects a new set of data rows in the same way\"\n",
        "    normdata = data - norminfo['centre']\n",
        "    try:\n",
        "        if norminfo['use_selfnormn']:\n",
        "            normdata -= np.mean(normdata, 1).reshape(-1,1)\n",
        "        return np.dot(normdata, norminfo['proj'])\n",
        "    except:\n",
        "        print(\"Matrix shapes were: data %s, norminfo['proj'] %s, np.mean(normdata, 1) %s\" % (np.shape(normdata), np.shape(norminfo['proj']), np.shape(np.mean(normdata, 1))))\n",
        "        raise\n",
        "\n",
        "def prepare_a_datum(datum, norminfo):\n",
        "    \"Typically used for new data; you use normalise_and_whiten() on your training data, then this method projects a single test datum in the same way\"\n",
        "    return prepare_data(datum.reshape(1, -1), norminfo).flatten()\n",
        "\n",
        "def unprepare_a_datum(datum, norminfo, uncentre=True):\n",
        "    \"The opposite of prepare_a_datum(). It can't fix selfnormn but otherwise.\"\n",
        "    datum = np.dot(datum, norminfo['invproj'])\n",
        "    if uncentre:\n",
        "        datum += norminfo['centre']\n",
        "    return datum\n",
        "\n",
        "###############################################\n",
        "\n",
        "        \n",
        "\n"
      ],
      "id": "visible-performer",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdRHEdon2XE-"
      },
      "source": [
        "## Examples from Dan Stowell"
      ],
      "id": "ZdRHEdon2XE-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjcnOtWR2VrW"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    import matplotlib\n",
        "    # matplotlib.use('PDF') # http://www.astrobetter.com/plotting-to-a-file-in-python/\n",
        "    import matplotlib.pyplot as plt\n",
        "    %matplotlib inline\n",
        "\n",
        "    km = OSKmeans(3, 2)\n",
        "    print(\"Centroids:\")\n",
        "    print(km.centroids)\n",
        "    for _ in range(20):\n",
        "        km.update([1,0])\n",
        "        km.update([0,1])\n",
        "        km.update([-1,0])\n",
        "    print(\"Centroids:\")\n",
        "    print(km.centroids)\n",
        "\n",
        "    ######################################\n",
        "    # Synthetic example: just for illustration purposes, we will create 3 2D clumps sampled\n",
        "    # from gaussians on angle and log-magnitude, and learn 10 means\n",
        "\n",
        "    truecentroids = [ # anglemean, anglesd, logmagmean, logmagsd\n",
        "        [1.0, 0.1, 1.0, 0.35],\n",
        "        [2.0, 0.3, 1.0, 0.2],\n",
        "        [4.0, 0.5, 0.7, 0.2],\n",
        "    ]\n",
        "    samples = [[] for _ in truecentroids]\n",
        "    np.random.seed(12345)\n",
        "    km = OSKmeans(10, 2)\n",
        "    for index in range(10000):\n",
        "        # sample from cluster index % 3\n",
        "        whichclust = index % len(truecentroids)\n",
        "        angle = np.random.normal(truecentroids[whichclust][0], truecentroids[whichclust][1])\n",
        "        magnitude = np.exp(np.random.normal(truecentroids[whichclust][2], truecentroids[whichclust][3]))\n",
        "        datum = [np.sin(angle) * magnitude, np.cos(angle) * magnitude]\n",
        "        # store that to the data list, along with its true identity\n",
        "        if index < 500:\n",
        "            samples[whichclust].append(datum)\n",
        "        # run it through kmeans\n",
        "        km.update(datum)\n",
        "\n",
        "    for plotlbl, showcentroids in [['datacent', True], ['data', False]]:\n",
        "        ucircle = plt.Circle((0,0),1, color=[0.9]*3, fill=False)\n",
        "        ax = plt.gca()\n",
        "        ax.cla() # clear things for fresh plot\n",
        "        ax.set_xlim((-4,4))\n",
        "        ax.set_ylim((-4,4))\n",
        "        ax.set_aspect('equal', 'box')\n",
        "        ax.axis('off')\n",
        "        fig = plt.gcf()\n",
        "        fig.gca().add_artist(ucircle)\n",
        "        \n",
        "        for sampleset in samples:\n",
        "            plt.plot([datum[0] for datum in sampleset], [datum[1] for datum in sampleset], '.')\n",
        "        if showcentroids:\n",
        "            plt.plot([datum[0] for datum in km.centroids], [datum[1] for datum in km.centroids], 'kx')\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.axvline(0, color=[0.7] * 3)\n",
        "        plt.axhline(0, color=[0.7] * 3)\n",
        "        # plt.savefig(\"%s/oskmeansexample-%s.pdf\" % ('.', plotlbl))\n",
        "        # plt.clf()\n",
        "        plt.show()"
      ],
      "id": "EjcnOtWR2VrW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VurayMKa2cjs"
      },
      "source": [
        "## Bird-song code"
      ],
      "id": "VurayMKa2cjs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-YMAdxU2sOI"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "import librosa\n",
        "import librosa.display\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tqdm.notebook import tqdm\n"
      ],
      "id": "O-YMAdxU2sOI",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weighted-ebony",
        "outputId": "dd798a6c-0d2d-4563-fc53-74b57704e1b8"
      },
      "source": [
        "# For Colab only\n",
        "# 4/1AY0e-g6CTocvBK45PWlJu2ycBuOMTgu36b-VZgpnCXitW_Vy3ckpGGBzur8\n",
        "from google.colab import drive\n",
        "drive.mount('content')\n",
        "# /content/content/MyDrive/bird-songs/audio"
      ],
      "id": "weighted-ebony",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG3U_ZmA2hlm",
        "outputId": "28344fae-6b62-454b-8952-d09a24898bba"
      },
      "source": [
        "# Check whether google drive mounted\n",
        "path = '/content/content/MyDrive/bird-songs/'\n",
        "if os.path.isdir(path) == True:\n",
        "    print('Google Drive Mounted')\n",
        "    run_on_colab = True\n",
        "else:\n",
        "    print('Using local drive')\n",
        "    run_on_colab = False\n",
        "    "
      ],
      "id": "qG3U_ZmA2hlm",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google Drive Mounted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbCBaqmS2qw7",
        "outputId": "05aeff0e-b8a1-4be4-bfcb-09fe24d9b0a2"
      },
      "source": [
        "def get_audio_filenames(audio_folder):\n",
        "    '''Create a list of audio files in the provided folder'''\n",
        "    \n",
        "    audio_files = []\n",
        "    for fn_mp3 in glob.glob(f\"{audio_folder}*\"):\n",
        "        audio_files.append(fn_mp3)\n",
        "    return audio_files\n",
        "\n",
        "if run_on_colab ==True:\n",
        "    audio_filenames = get_audio_filenames('/content/content/MyDrive/bird-songs/audio/')\n",
        "    print(f'{len(audio_filenames)} audio files appended to list from Google Drive')\n",
        "else:\n",
        "    audio_filenames = get_audio_filenames('./')\n",
        "    print(f'{len(audio_filenames)} audio files appended to list from local drive')\n",
        "\n",
        "print (audio_filenames[0])"
      ],
      "id": "YbCBaqmS2qw7",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "500 audio files appended to list from Google Drive\n",
            "/content/content/MyDrive/bird-songs/audio/418340.mp3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780,
          "referenced_widgets": [
            "e987300ade6d4c4d9b6d4d09228a4d9b",
            "2d0e50308adf42da8f5f65963a1af50a",
            "3059921ee908434382c0b9748dc6a767",
            "bcf7aa0066dc45f7b6341b18fed44a88",
            "2af9251e1d3c4460b6ae2fda13ae4724",
            "ed94f88f9ece40b795b9e7a33e8c4033",
            "61dade7ba51a49a9b1e88180ccdc120a",
            "a65b35a1d95f4fadb81979a94bf32aed"
          ]
        },
        "id": "scZtFoSy3AR4",
        "outputId": "ade24432-5ebb-4fad-a9f8-d39f39c12270"
      },
      "source": [
        "def create_mel_spectral_features(sample_rate=22050):\n",
        "    ''' '''\n",
        "\n",
        "    # # Create df to hold mfcc spectral features\n",
        "    # df = pd.DataFrame(columns = ['mel_avg' + str(item+1) for item in list(range(128))] +\n",
        "    #                             ['mel_std' + str(item+1) for item in list(range(128))], \n",
        "    #                   index = df_xeno_canto['filename'])\n",
        "        \n",
        "    km = OSKmeans(10, 40)\n",
        "    \n",
        "    for audio_filename in tqdm(audio_filenames[:1]):  # tqdm()\n",
        "        \n",
        "        \n",
        "        y, sr = librosa.load(audio_filename, sr=sample_rate, mono=True)\n",
        "        mels = librosa.feature.melspectrogram(y=y, sr=sample_rate, n_mels=40, n_fft=2048, hop_length=1024, fmin=500)\n",
        "        # trans_mels = np.transpose(mels)\n",
        "        normalized = normalise_and_whiten(mels)\n",
        "        # for mel in np.transpose(mels):\n",
        "            \n",
        "            # km.update(mel.tolist())\n",
        "        # mels_means = [np.mean(item) for item in mels]\n",
        "        # mels_stds = [np.std(item) for item in mels]\n",
        "        # # print(\"mel means:\", mels_means, \"\\nmel std deviations:\", mels_stds)\n",
        "        # df.loc[audio_filename] = mels_means + mels_stds\n",
        "  \n",
        "    return  normalized   #km.centroids\n",
        "\n",
        "\n",
        "\n",
        "km_centroids = create_mel_spectral_features()\n",
        "\n",
        "print(km_centroids[1])"
      ],
      "id": "scZtFoSy3AR4",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e987300ade6d4c4d9b6d4d09228a4d9b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "computing number of components to retain 0.99 of the variance...\n",
            "number of components to retain: 5 of 302\n",
            "\n",
            "[[ 2.5221014   0.5940636  -1.7141949   2.036521    4.8027472 ]\n",
            " [-1.0775133   1.5378896   3.3684664  -0.15732282  0.41297337]\n",
            " [-0.41843796  0.1489644   0.04474393  0.09934345 -0.3024501 ]\n",
            " [-0.6961894  -0.7359495  -0.06900632 -0.10577793 -0.2980328 ]\n",
            " [ 0.76499236 -3.8282583  -0.8043725  -1.4829034   0.19009735]\n",
            " [-0.40085566  1.4641328  -1.3264047  -5.347243    1.8346566 ]\n",
            " [-1.9132217   1.3682431  -0.18116818 -0.74957263  0.26416355]\n",
            " [ 1.5985452   1.6811472   3.6475594  -0.06622403  1.1093692 ]\n",
            " [-4.8976088  -0.9543925  -0.21748428  1.6018826   2.4026184 ]\n",
            " [ 0.45611295 -3.698629    2.7847853  -0.79100984  1.1463335 ]\n",
            " [ 0.37063903 -0.5663375   0.8648648  -0.4799514   0.18417619]\n",
            " [-0.09203159  0.12346564 -0.17365804  0.11983775 -0.36809045]\n",
            " [ 0.13705249  0.09425705 -0.21511577  0.17477697 -0.3979947 ]\n",
            " [ 0.12970994  0.09456997 -0.21992105  0.18710397 -0.4030299 ]\n",
            " [ 0.13095985  0.10106474 -0.22058403  0.1897147  -0.40550095]\n",
            " [ 0.12840445  0.10143236 -0.22186537  0.19034031 -0.40566874]\n",
            " [ 0.12831005  0.10122982 -0.22077875  0.18937497 -0.40591782]\n",
            " [ 0.12859048  0.10132743 -0.22117455  0.18940358 -0.4060669 ]\n",
            " [ 0.11994796  0.10010365 -0.22110137  0.19034351 -0.40623504]\n",
            " [ 0.13109547  0.10400615 -0.2218064   0.19084206 -0.40712088]\n",
            " [ 0.15889044  0.11262292 -0.22717188  0.19599304 -0.40900326]\n",
            " [ 0.18608446  0.1165493  -0.23433863  0.19827737 -0.41230363]\n",
            " [ 0.15676181  0.10928328 -0.2269257   0.19279602 -0.40877536]\n",
            " [ 0.15130506  0.10892153 -0.22569916  0.19309585 -0.4073986 ]\n",
            " [ 0.14740135  0.10645165 -0.2228257   0.19085638 -0.40621674]\n",
            " [ 0.13292322  0.09912707 -0.22064221  0.19051576 -0.4066693 ]\n",
            " [ 0.1311856   0.10066623 -0.22183563  0.19022624 -0.40668613]\n",
            " [ 0.13060512  0.10132679 -0.22188793  0.19000073 -0.4065455 ]\n",
            " [ 0.13040188  0.10122557 -0.22176386  0.18996935 -0.40646967]\n",
            " [ 0.13063711  0.10147567 -0.2218959   0.1901503  -0.4064634 ]\n",
            " [ 0.12934543  0.10102502 -0.22167315  0.1898773  -0.40645388]\n",
            " [ 0.12926188  0.10097714 -0.22166203  0.18985733 -0.406442  ]\n",
            " [ 0.12938108  0.1010365  -0.2217167   0.18986684 -0.40645787]\n",
            " [ 0.12955453  0.10105711 -0.22174129  0.18990047 -0.40647826]\n",
            " [ 0.12938748  0.10101114 -0.22169575  0.18986861 -0.40645143]\n",
            " [ 0.12928088  0.10098629 -0.22166969  0.18986186 -0.4064455 ]\n",
            " [ 0.12925003  0.10098755 -0.22165321  0.18985136 -0.40643817]\n",
            " [ 0.12923548  0.10098103 -0.22165954  0.18985179 -0.4064422 ]\n",
            " [ 0.12924908  0.10098058 -0.22166592  0.18985446 -0.4064428 ]\n",
            " [ 0.12925942  0.10097291 -0.22167054  0.1898509  -0.40644318]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgVAE5mv5_J6"
      },
      "source": [
        ""
      ],
      "id": "VgVAE5mv5_J6",
      "execution_count": null,
      "outputs": []
    }
  ]
}