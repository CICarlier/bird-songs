{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ethical-stomach",
   "metadata": {},
   "source": [
    "# Spherical K-Means  \n",
    "\n",
    "Code taken from:  \n",
    "Dan Stowell and Mark Plumbley (2014), Automatic large-scale classification of bird sounds is strongly improved by unsupervised feature learning  \n",
    "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4106198/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "visible-performer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centroids:\n",
      "[[-0.47966233 -0.87745316]\n",
      " [-0.35930269 -0.93322108]\n",
      " [-0.99523609 -0.09749424]]\n",
      "Centroids:\n",
      "[[-0.94889111 -0.31560365]\n",
      " [-0.92860906 -0.37105957]\n",
      " [-0.99248995 -0.12232618]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Online spherical Hartigan k-means\n",
    "# Dan Stowell, Jan 2014\n",
    "\n",
    "# This file in particular is published under the following open licence:\n",
    "#######################################################################################\n",
    "# Copyright (c) 2014, Dan Stowell and Queen Mary University of London\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n",
    "#\n",
    "# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "#######################################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class OSKmeans:\n",
    "    \"\"\"\n",
    "    This class implements online Hartigan k-means, except:\n",
    "     - spherical k-means, i.e. the centroids are constrained to L2-norm of 1\n",
    "     - centroids are initialised randomly on the sphere\n",
    "     - a weight-offset is used to ensure that the random inits aren't simply blown away as soon as the first data-point comes in\n",
    "\n",
    "    Further reading:\n",
    "     - Appendix B of         http://cseweb.ucsd.edu/~bmcfee/papers/bmcfee_dissertation.pdf\n",
    "     - Coates et al 2012, Learning feature representations with k-means\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k, d, weightoffset=2):\n",
    "        \"\"\"\n",
    "        k = num centroids\n",
    "        d = num dimensions\n",
    "        weightoffset: how \"reluctant\" the centroids are to move at first. set to 0 for traditional initialisation of centroids as random data.\n",
    "        \"\"\"\n",
    "        self.hitcounts =          [weightoffset               for _ in range(k)]     # shape (k)\n",
    "        self.centroids = np.array([spherical_random_sample(d) for _ in range(k)])    # shape (k, d)\n",
    "        self.k = k\n",
    "        self.d = d\n",
    "\n",
    "    def weightednearest(self, datum):\n",
    "        \"Find the index of the best-matching centroid, using the Hartigan count-weighting scheme in combination with cosine similarity\"\n",
    "        # find the best-matching centroid (note that it's not pure distance, but count-weighted)\n",
    "        bestindex = 0\n",
    "        bestval   = np.inf\n",
    "        bestcosinesim   = 0\n",
    "        datum = unit_normalise(datum)\n",
    "\n",
    "        try:\n",
    "            cosinesims = np.dot(self.centroids, datum.T)  # cosine similarity, shape (k)\n",
    "            cosinedists = 1. - cosinesims  # converted to a distance for the benefit of the count-weighting\n",
    "            cosinedists *= [self.hitcounts[which]/float(self.hitcounts[which]+1) for which in range(self.k)]  # Hartigan count-weighting\n",
    "        except:\n",
    "            print(\"Matrix shapes were: centroids %s, datum %s\" % (np.shape(self.centroids), np.shape(datum)))\n",
    "            raise\n",
    "\n",
    "        bestindex = np.argmin(cosinedists)\n",
    "        bestcosinesim = float(cosinesims[bestindex])\n",
    "        return (bestindex, bestcosinesim)  # NOTE: the dot product returned here is with the normalised input, i.e. with its unit vector.\n",
    "\n",
    "    def update(self, datum):\n",
    "        \"Feed individual data into this to perform learning\"\n",
    "        datum = np.array(datum)\n",
    "        bestindex, dotprod = self.weightednearest(datum)\n",
    "\n",
    "        # update the centroid, including the renormalisation for sphericalness\n",
    "        centroid = self.centroids[bestindex]\n",
    "        hitcount = self.hitcounts[bestindex]\n",
    "        newcentroid = unit_normalise(centroid * hitcount + datum * dotprod)\n",
    "        self.centroids[bestindex] = newcentroid\n",
    "\n",
    "        # update the hit count\n",
    "        self.hitcounts[bestindex] = hitcount + 1\n",
    "        # return the index, and the amount by which the centroid has changed (useful for monitoring)\n",
    "        return (bestindex, np.sqrt(((centroid-newcentroid)**2).sum()))\n",
    "\n",
    "    def train_batch(self, whitedata, niters=10, verbose=True):\n",
    "        \"If you have a batch of data, rather than streamed, this method is a convenience to train using 'niters' iterations of the shuffled data.\"\n",
    "        shuffle_indices = range(len(whitedata))\n",
    "        for whichiter in range(niters):\n",
    "            if verbose:\n",
    "                print (f\"Iteration {whichiter}\")\n",
    "            np.random.shuffle(shuffle_indices)\n",
    "            for upindex, index in enumerate(shuffle_indices):\n",
    "                self.update(whitedata[index])\n",
    "\n",
    "    def sort_centroids(self):\n",
    "        \"\"\"Not needed! Purely cosmetic, for comprehensibility of certain plots. Reorders the centroids by an arbitrary spectral-centroid-like measure.\n",
    "        Note that for 2D features such as chrmfull, the ordering may not have obvious sense since it operates on the vectorised data.\"\"\"\n",
    "        if False:\n",
    "            binlist = np.arange(len(self.centroids[0]))\n",
    "            sortifier = np.argsort([np.sum(centroid * binlist)/np.sum(centroid) for centroid in self.centroids])\n",
    "        else:\n",
    "            # new sorting method, using a simple approximation to TSP to organise the centroids so that close ones are close.\n",
    "            similarities = np.dot(self.centroids, self.centroids.T)\n",
    "            #print \"sort_centroids() -- similarities is of shape %s\" % str(similarities.shape)\n",
    "\n",
    "            # Now, starting arbitrarily from index 0, we \"grow each end\" iteratively\n",
    "            pathends = [[0], [0]]\n",
    "            worstarcpos = None\n",
    "            worstarcdot = 1\n",
    "            availablenodes = range(1, len(similarities))\n",
    "            whichend = 0\n",
    "            #print(\"---------------------------------------------------\")\n",
    "            while len(availablenodes) != 0:\n",
    "                #print(\"availablenodes (length %i): %s\" % (len(availablenodes), str(availablenodes)))\n",
    "                whichend = 1 - whichend  # alternate between one and zero\n",
    "                frm = pathends[whichend][-1]\n",
    "                # iterate over the nodes that are so-far unused, finding the best one to join on\n",
    "                bestpos = availablenodes[0]\n",
    "                bestdot = -1\n",
    "                for too in availablenodes:\n",
    "                    curdot = similarities[frm, too]\n",
    "                    if curdot > bestdot:\n",
    "                        bestpos = too\n",
    "                        bestdot = curdot\n",
    "                # check if this is the worst arc so far made\n",
    "                if bestdot < worstarcdot:\n",
    "                    worstarcdot = bestdot\n",
    "                    worstarcpos = (whichend, len(pathends[whichend]))\n",
    "                # append this arc\n",
    "                pathends[whichend].append(bestpos)\n",
    "                # remove the chosen one from availablenodes\n",
    "                #print(\" bestpos: %i, dot %g\" % (bestpos, bestdot))\n",
    "                availablenodes.remove(bestpos)\n",
    "\n",
    "            # finally, we need to check the join-the-two-ends arc to see if it's the worst\n",
    "            curdot = similarities[pathends[0][-1], pathends[1][-1]]\n",
    "            # we can choose the worst arc found as the place to split the circuit; and create the sortifier\n",
    "            if curdot < worstarcdot:\n",
    "                # we will snip the way the paths themselves snipped\n",
    "                sortifier = pathends[0][::-1] + pathends[1][1:]\n",
    "            else:\n",
    "                # we will snip at some location inside one of the lists, and rejoin\n",
    "                (snipwhich, snipwhere) = worstarcpos\n",
    "                sortifier = pathends[snipwhich][snipwhere::-1] + pathends[1-snipwhich][1:] + pathends[snipwhich][:snipwhere:-1]\n",
    "            if sorted(sortifier) != range(len(similarities)):\n",
    "                print(\"pathends: %s\" % str(pathends))\n",
    "                raise RuntimeError(\"sorted(sortifier) != range(len(similarities)): sorted(%s) != range(%s)\") % (sortifier, len(similarities))\n",
    "            #print(\"Simple TSP method: decided on the following sortifier: %s\" % str(sortifier))\n",
    "\n",
    "        self.centroids = np.array([self.centroids[index] for index in sortifier])\n",
    "        self.hitcounts =          [self.hitcounts[index] for index in sortifier]\n",
    "\n",
    "\n",
    "    def relative_entropy_hitcounts(self):\n",
    "        \"The entropy over the centroid hitcounts is a useful measure of how well they are used. Here we normalise it against the ideal uniform entropy\"\n",
    "        h = 0.\n",
    "        tot = float(np.sum(self.hitcounts))\n",
    "        for hitcount in self.hitcounts:\n",
    "            p = hitcount / tot\n",
    "            h -= p * np.log(p)\n",
    "        h_unif = np.log(len(self.hitcounts))\n",
    "        return h / h_unif\n",
    "\n",
    "    def reconstruct1(self, datum, whichcentroid):\n",
    "        \"Reconstruct an input datum using a single indexed centroid\"\n",
    "        return self.centroids[whichcentroid] * np.dot(self.centroids[whichcentroid], datum)\n",
    "\n",
    "    def dotproducts(self, data):\n",
    "        'Used by thresholded_dotproducts(); subclasses may overwrite'\n",
    "        return np.dot(data, self.centroids.T)\n",
    "\n",
    "    def thresholded_dotproducts(self, data, threshold=0.0):\n",
    "        \"One possible 'feature' set based on centroids is this, the thresholded dot products. Supply a matrix as one row per datum.\"\n",
    "        try:\n",
    "            return np.maximum(0, self.dotproducts(data) - threshold)\n",
    "        except:\n",
    "            print(\"Matrix shapes were: centroids %s, data %s\" % (np.shape(self.centroids), np.shape(data)))\n",
    "            raise\n",
    "\n",
    "###############################################\n",
    "def spherical_random_sample(d):\n",
    "    vec = np.random.normal(size=d)\n",
    "    return unit_normalise(vec)\n",
    "\n",
    "def unit_normalise(vec):\n",
    "    return vec / np.sqrt((vec ** 2).sum())\n",
    "\n",
    "###############################################\n",
    "# useful functions for whitening a dataset\n",
    "\n",
    "def normalise_and_whiten(data, retain=0.99, bias=1e-8, use_selfnormn=True, min_ndims=1):\n",
    "    \"Use this to prepare a training set before running through OSKMeans\"\n",
    "    mean = np.mean(data, 0)\n",
    "    normdata = data - mean\n",
    "\n",
    "    if use_selfnormn:\n",
    "        for i in range(normdata.shape[0]):\n",
    "            normdata[i] -= np.mean(normdata[i])\n",
    "\n",
    "    # this snippet is based on an example by Sander Dieleman\n",
    "    cov = np.dot(normdata.T, normdata) / normdata.shape[0]\n",
    "    eigs, eigv = np.linalg.eigh(cov) # docs say the eigenvalues are NOT NECESSARILY ORDERED, but this seems to be the case in practice...\n",
    "    print (f\"computing number of components to retain {retain:.2f} of the variance...\")\n",
    "    normed_eigs = eigs[::-1] / np.sum(eigs) # maximal value first\n",
    "    eigs_sum = np.cumsum(normed_eigs)\n",
    "    num_components = max(min_ndims, np.argmax(eigs_sum > retain)) # argmax selects the first index where eigs_sum > retain is true\n",
    "    print(f\"number of components to retain: {num_components} of {len(eigs)}\")\n",
    "    P = eigv.astype('float32') * np.sqrt(1.0/(eigs + bias)) # PCA whitening\n",
    "    P = P[:, -num_components:] # truncate transformation matrix\n",
    "\n",
    "    whitedata = np.dot(normdata, P)\n",
    "    invproj = np.linalg.pinv(P)\n",
    "    return ({'centre': mean, 'proj': P, 'ncomponents': num_components, 'invproj': invproj, 'use_selfnormn': use_selfnormn}, whitedata)\n",
    "\n",
    "def prepare_data(data, norminfo):\n",
    "    \"Typically used for new data; you use normalise_and_whiten() on your training data, then this method projects a new set of data rows in the same way\"\n",
    "    normdata = data - norminfo['centre']\n",
    "    try:\n",
    "        if norminfo['use_selfnormn']:\n",
    "            normdata -= np.mean(normdata, 1).reshape(-1,1)\n",
    "        return np.dot(normdata, norminfo['proj'])\n",
    "    except:\n",
    "        print(\"Matrix shapes were: data %s, norminfo['proj'] %s, np.mean(normdata, 1) %s\" % (np.shape(normdata), np.shape(norminfo['proj']), np.shape(np.mean(normdata, 1))))\n",
    "        raise\n",
    "\n",
    "def prepare_a_datum(datum, norminfo):\n",
    "    \"Typically used for new data; you use normalise_and_whiten() on your training data, then this method projects a single test datum in the same way\"\n",
    "    return prepare_data(datum.reshape(1, -1), norminfo).flatten()\n",
    "\n",
    "def unprepare_a_datum(datum, norminfo, uncentre=True):\n",
    "    \"The opposite of prepare_a_datum(). It can't fix selfnormn but otherwise.\"\n",
    "    datum = np.dot(datum, norminfo['invproj'])\n",
    "    if uncentre:\n",
    "        datum += norminfo['centre']\n",
    "    return datum\n",
    "\n",
    "###############################################\n",
    "if __name__ == \"__main__\":\n",
    "    import matplotlib\n",
    "    matplotlib.use('PDF') # http://www.astrobetter.com/plotting-to-a-file-in-python/\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    km = OSKmeans(3, 2)\n",
    "    print(\"Centroids:\")\n",
    "    print(km.centroids)\n",
    "    for _ in range(20):\n",
    "        km.update([1,0])\n",
    "        km.update([0,1])\n",
    "        km.update([-1,0])\n",
    "    print(\"Centroids:\")\n",
    "    print(km.centroids)\n",
    "\n",
    "    ######################################\n",
    "    # Synthetic example: just for illustration purposes, we will create 3 2D clumps sampled from gaussians on angle and log-magnitude, and learn 10 means\n",
    "\n",
    "    truecentroids = [ # anglemean, anglesd, logmagmean, logmagsd\n",
    "        [1.0, 0.1, 1.0, 0.35],\n",
    "        [2.0, 0.3, 1.0, 0.2],\n",
    "        [4.0, 0.5, 0.7, 0.2],\n",
    "    ]\n",
    "    samples = [[] for _ in truecentroids]\n",
    "    np.random.seed(12345)\n",
    "    km = OSKmeans(10, 2)\n",
    "    for index in range(10000):\n",
    "        # sample from cluster index % 3\n",
    "        whichclust = index % len(truecentroids)\n",
    "        angle = np.random.normal(truecentroids[whichclust][0], truecentroids[whichclust][1])\n",
    "        magnitude = np.exp(np.random.normal(truecentroids[whichclust][2], truecentroids[whichclust][3]))\n",
    "        datum = [np.sin(angle) * magnitude, np.cos(angle) * magnitude]\n",
    "        # store that to the data list, along with its true identity\n",
    "        if index < 500:\n",
    "            samples[whichclust].append(datum)\n",
    "        # run it through kmeans\n",
    "        km.update(datum)\n",
    "\n",
    "    for plotlbl, showcentroids in [['datacent', True], ['data', False]]:\n",
    "        ucircle = plt.Circle((0,0),1, color=[0.9]*3, fill=False)\n",
    "        ax = plt.gca()\n",
    "        ax.cla() # clear things for fresh plot\n",
    "        ax.set_xlim((-4,4))\n",
    "        ax.set_ylim((-4,4))\n",
    "        ax.set_aspect('equal', 'box')\n",
    "        ax.axis('off')\n",
    "        fig = plt.gcf()\n",
    "        fig.gca().add_artist(ucircle)\n",
    "\n",
    "        for sampleset in samples:\n",
    "            plt.plot([datum[0] for datum in sampleset], [datum[1] for datum in sampleset], '.')\n",
    "        if showcentroids:\n",
    "            plt.plot([datum[0] for datum in km.centroids], [datum[1] for datum in km.centroids], 'kx')\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.axvline(0, color=[0.7] * 3)\n",
    "        plt.axhline(0, color=[0.7] * 3)\n",
    "        plt.savefig(\"%s/oskmeansexample-%s.pdf\" % ('.', plotlbl))\n",
    "        plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-youth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
